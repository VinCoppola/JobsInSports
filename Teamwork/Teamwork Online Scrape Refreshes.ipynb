{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76de6496",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    "Contained below is a functional script that webscrapes wikipedia and Teamwork Online - the best sports database available. The script is as follows:\n",
    "\n",
    "First the libraries are loaded for scraping and data cleaning. Then Teamwork Online is scraped through a variety of user -defined functions and passed into the list: job_list. \n",
    "\n",
    "This list is then made into a dataframe and validated through a series of agile development cycles which included visualizing the data table at each step. In the end the final table is saved into job_posting_teamwork_df. Cleaning included many partions, replacements, typecasting and reindexing as well as other steps.\n",
    "\n",
    "Some of the data was passed into other dataframes such as job_requirements_df_final which contains an exploded list of job requirements and qualifications scraped from Teamwork Online. Another dataframe made was called Company_Team_df and contained the distinct companies and an encoded ID number. \n",
    "\n",
    "Further scraping came into play when all major leagues' (MLS, MLB, NFL, NHL, and NBA) wiki pages were scraped to get all team information. This data was then cleaned and merged with the actual companies so that those that did have a team match would have that info. Many NULLS occured and were cleaned as well as possible. \n",
    "\n",
    "Finally, the database was connected to and all data was succesfully imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install requests_html\n",
    "#from requests_html import HTMLSession\n",
    "import random\n",
    "import re\n",
    "#from nltk import bigrams\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "#! pip install wordcloud\n",
    "#from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dict1, dict2):\n",
    "    return(dict2.update(dict1))\n",
    "\n",
    "def extract(page):\n",
    "    url = f'https://www.teamworkonline.com/jobs-in-sports?page={page}'\n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r = requests.get(url,headers)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def extract_inner(link_ext):\n",
    "    url_inner = 'https://www.teamworkonline.com' + link_ext\n",
    "    \n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r_inner = requests.get(url_inner,headers)\n",
    "    soup_inner = BeautifulSoup(r_inner.content,'html.parser')\n",
    "    return(soup_inner)\n",
    "\n",
    "def transform(soup):\n",
    "    divs = soup.find_all('div',class_ = 'result-item recent-job')\n",
    "    for job in divs:\n",
    "        title = job.find('h3',class_='base-font').text.strip()\n",
    "        job_exception = job.find_all('span',class_ = 'icon-bullet__content icon-bullet__content--recent-job-card')   \n",
    "        for i in job_exception:\n",
    "            \n",
    "            if i.text.endswith('Jobs'):\n",
    "                company_temp = i.text.replace(' Jobs','')\n",
    "            else:\n",
    "                location_temp = i.text.replace('Jobs in ',' (')\n",
    "        \n",
    "        link_ext = job.a['href']\n",
    "        \n",
    "        #details = []\n",
    "        more_info = extract_inner(link_ext)\n",
    "        try:\n",
    "            divs_inner_1 = more_info.find('div',class_ = 'opportunity-preview__body').find_all('ul')\n",
    "            details = []\n",
    "        \n",
    "            for info in divs_inner_1:\n",
    "                for i in (info.find_all('li')):\n",
    "                    details.append(i.text.strip())\n",
    "            \n",
    "        except:\n",
    "            details= []\n",
    "            \n",
    "        try:\n",
    "            full_job = (more_info.find('h1',class_ = 'opportunity-preview__title').text)\n",
    "        except:\n",
    "            full_job = (title + '-' + company_temp + location_temp + ')')\n",
    "            # Up until - is job, after dash to ( is company and (INSIDE parenthese is location)\n",
    "        \n",
    "        job = {\n",
    "            'title': title,\n",
    "            'job_info': full_job,\n",
    "            'url': 'https://www.teamworkonline.com' + link_ext,\n",
    "            'details': details,\n",
    "            'scrape_datetime': datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        joblist.append(job)\n",
    "    \n",
    "    return\n",
    "\n",
    "joblist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f44c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ((1,3),(3,5),(5,7),(7,9),(9,11))\n",
    "for i in pages:\n",
    "    for j in range(i[0],i[1]):\n",
    "        c=extract(j)\n",
    "        transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00322d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = MySQLdb.connect(host=\"localhost\" , user=\"root\" , passwd=\"Pps11844\")\n",
    "cursor = database.cursor()\n",
    "\n",
    "def execute_query(query_statement):\n",
    "    try:\n",
    "        cursor.execute(query_statement);\n",
    "        database.commit();\n",
    "        print(\"Data is Succefully Inserted\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        database.rollback();\n",
    "        print(\"The  Exception Occured : \", e)\n",
    "\n",
    "execute_query(\"USE JobsinSports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_df_posting = pd.read_sql('select * from job_posting',database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92676752",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_df_companies = pd.read_sql('select * from company_team',database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT MAX(company_ID) FROM company_team;\")\n",
    "result = cursor.fetchone();\n",
    "max_comp_ID = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT MAX(job_ID) FROM job_posting;\")\n",
    "result2 = cursor.fetchone();\n",
    "max_job_ID = result2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_df_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## USED FOR INITIAL SCRAPE ###################################################\n",
    "# Creating and cleaning job data table\n",
    "job_posting_teamwork = pd.DataFrame(joblist)\n",
    "\n",
    "for i,j in job_posting_teamwork.iterrows():\n",
    "    if j['title'] in (j['job_info']):\n",
    "        j['job_info'] = j['job_info'].replace(j['title'],'')\n",
    "\n",
    "job_posting_teamwork[\"Location\"] = (job_posting_teamwork[\"job_info\"].str.partition(\"(\")[2]).str.replace(\")\",\"\").str.replace(' · ',', ').str.replace('· ',', ').str.strip()\n",
    "job_posting_teamwork[\"Company\"] = job_posting_teamwork[\"job_info\"].str.partition(\"(\")[0].str.partition(\"-\")[2].str.strip()\n",
    "\n",
    "job_posting_teamwork['job_city'] = job_posting_teamwork['Location'].str.partition(\",\")[0]\n",
    "job_posting_teamwork['job_state'] = job_posting_teamwork['Location'].str.partition(\",\")[2]\n",
    "\n",
    "job_posting_teamwork = job_posting_teamwork.drop([\"job_info\",\"Location\"],axis=1)\n",
    "for i,j in job_posting_teamwork.iterrows():\n",
    "    if(j[\"Company\"] == \"Oakland A's\"):\n",
    "        j[\"Company\"] = \"Oakland Athletics\"\n",
    "    \n",
    "    elif(j[\"Company\"] == \"NYCFC\"):\n",
    "            j[\"Company\"] = \"New York City FC\"\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i,j in job_posting_teamwork.iterrows():\n",
    "    if((j['title'] in SQL_df_posting['job_title'].values) and (j['Company'] in SQL_df_companies['company_name'].values)):\n",
    "        job_posting_teamwork = job_posting_teamwork.drop(index = i,axis = 1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "number = LabelEncoder()\n",
    "\n",
    "job_posting_teamwork[\"company_ID\"] = number.fit_transform(job_posting_teamwork[\"Company\"].astype('str'))\n",
    "job_posting_teamwork.loc[job_posting_teamwork['company_ID'] == 0,'company_ID'] = (max(job_posting_teamwork['company_ID'])+1)\n",
    "\n",
    "job_posting_teamwork['job_ID'] = np.arange(max_job_ID + 1, len(job_posting_teamwork) + max_job_ID+1)\n",
    "\n",
    "job_posting_teamwork['posting_source_ID'] = 2\n",
    "job_posting_teamwork['posting_datetime'] = 'NA'\n",
    "job_posting_teamwork['application_deadline'] = 'Unknown'\n",
    "job_posting_teamwork['salary'] = 'Unknown'\n",
    "job_posting_teamwork['scrape_datetime'] = pd.to_datetime(job_posting_teamwork['scrape_datetime'])\n",
    "\n",
    "job_posting_teamwork = job_posting_teamwork.rename(columns = {'title': 'job_title', 'url': 'posting_link'})\n",
    "\n",
    "job_posting_teamwork_df = job_posting_teamwork.reindex(columns = ['job_ID','job_title','Company',\"company_ID\",'posting_source_ID','posting_datetime','scrape_datetime','application_deadline','salary','job_city','job_state','details','posting_link'])\n",
    "\n",
    "# Creating Company Table\n",
    "Company_Team = pd.DataFrame(job_posting_teamwork[['company_ID','Company']])\n",
    "Company_Team_df = Company_Team.drop_duplicates()\n",
    "\n",
    "# Creating the requirements table\n",
    "job_requirements_df = pd.DataFrame(job_posting_teamwork_df[['job_ID','details']])\n",
    "job_requirements_df_final = job_requirements_df.assign(temp = job_requirements_df.details.str.split(\",\")).explode('details').drop('temp',axis=1)\n",
    "job_requirements_df_final['details'] = job_requirements_df_final['details'].str.replace(\"'\",\"\").str.replace('\"','')\n",
    "job_posting_teamwork_df = job_posting_teamwork_df.drop('details',axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "for i,j in Company_Team_df.iterrows():\n",
    "    if((j['Company'] in SQL_df_companies['company_name'].values)):\n",
    "        Company_Team_df.at[i,'company_ID'] = SQL_df_posting.loc[i,'company_ID']\n",
    "    else:\n",
    "        Company_Team_df.at[i,'company_ID'] = max_comp_ID + count\n",
    "        count = count + 1\n",
    "        \n",
    "job_posting_teamwork_df = pd.merge(job_posting_teamwork_df, Company_Team_df, left_on=\"Company\", right_on=\"Company\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7367fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_teamwork_df = job_posting_teamwork_df.rename(columns = {'company_ID_y': 'company_ID'})\n",
    "job_posting_teamwork_df = job_posting_teamwork_df.drop(['Company','company_ID_x'],axis = 1)\n",
    "job_posting_teamwork_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Company_Team_df.iterrows():\n",
    "    if((j['company_ID'] in SQL_df_companies['company_ID'].values) and (j['Company'] in SQL_df_companies['company_name'].values)):\n",
    "        Company_Team_df = Company_Team_df.drop(index = i,axis = 1)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources = pd.DataFrame({'source_ID': [2], 'source_name': ['Teamwork Online']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6093e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize connection to MYSQL\n",
    "database = MySQLdb.connect(host=\"localhost\" , user=\"root\" , passwd=\"Pps11844\")\n",
    "cursor = database.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_statement):\n",
    "    try:\n",
    "        cursor.execute(query_statement);\n",
    "        database.commit();\n",
    "        print(\"Data is Succefully Inserted\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        database.rollback();\n",
    "        print(\"The  Exception Occured : \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"USE JobsinSports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in job_requirements_df_final.iterrows():\n",
    "    execute_query('INSERT INTO Job_Requirements (job_ID, requirements) VALUES (%d, \"%s\")' % (j['job_ID'],j['details']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Sources.iterrows():\n",
    "    execute_query('INSERT INTO Sources (source_ID, source_name) VALUES (%d, \"%s\")' % (j['source_ID'],j['source_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Company_Team_df.iterrows():\n",
    "    execute_query('INSERT INTO Company_Team (company_ID, company_name) VALUES (%d, \"%s\")' % (j['company_ID'], j['Company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in job_posting_teamwork_df.iterrows():\n",
    "    execute_query('INSERT INTO Job_Posting (job_ID, job_title, company_ID, scraped_datetime, job_city, job_state, posting_url, source_identifier) VALUES (%d, \"%s\", %d, \"%s\", \"%s\", \"%s\", \"%s\", %d)' % (j['job_ID'], j['job_title'], j['company_ID'], j['scrape_datetime'], j['job_city'], j['job_state'], j['posting_link'], j['posting_source_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c27aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
