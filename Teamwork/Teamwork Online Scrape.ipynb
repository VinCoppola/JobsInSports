{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76de6496",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    "Contained below is a functional script that webscrapes wikipedia and Teamwork Online - the best sports database available. The script is as follows:\n",
    "\n",
    "First the libraries are loaded for scraping and data cleaning. Then Teamwork Online is scraped through a variety of user -defined functions and passed into the list: job_list. \n",
    "\n",
    "This list is then made into a dataframe and validated through a series of agile development cycles which included visualizing the data table at each step. In the end the final table is saved into job_posting_teamwork_df. Cleaning included many partions, replacements, typecasting and reindexing as well as other steps.\n",
    "\n",
    "Some of the data was passed into other dataframes such as job_requirements_df_final which contains an exploded list of job requirements and qualifications scraped from Teamwork Online. Another dataframe made was called Company_Team_df and contained the distinct companies and an encoded ID number. \n",
    "\n",
    "Further scraping came into play when all major leagues' (MLS, MLB, NFL, NHL, and NBA) wiki pages were scraped to get all team information. This data was then cleaned and merged with the actual companies so that those that did have a team match would have that info. Many NULLS occured and were cleaned as well as possible. \n",
    "\n",
    "Finally, the database was connected to and all data was succesfully imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install requests_html\n",
    "#from requests_html import HTMLSession\n",
    "import random\n",
    "import re\n",
    "#from nltk import bigrams\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "#! pip install wordcloud\n",
    "#from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to merge two dictionaries\n",
    "def merge(dict1, dict2):\n",
    "    return(dict2.update(dict1))\n",
    "\n",
    "## Function to extract the beautiful soup from link + pagenumber(s)\n",
    "def extract(page):\n",
    "    url = f'https://www.teamworkonline.com/jobs-in-sports?page={page}'\n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r = requests.get(url,headers)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "## Function to extract Beautiful Soup from inner links after scraped from header\n",
    "def extract_inner(link_ext):\n",
    "    url_inner = 'https://www.teamworkonline.com' + link_ext\n",
    "    \n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r_inner = requests.get(url_inner,headers)\n",
    "    soup_inner = BeautifulSoup(r_inner.content,'html.parser')\n",
    "    return(soup_inner)\n",
    "\n",
    "## Function to perform html parsing\n",
    "def transform(soup):\n",
    "    divs = soup.find_all('div',class_ = 'result-item recent-job')\n",
    "    for job in divs:\n",
    "        title = job.find('h3',class_='base-font').text.strip()\n",
    "        job_exception = job.find_all('span',class_ = 'icon-bullet__content icon-bullet__content--recent-job-card')   \n",
    "        for i in job_exception:\n",
    "            \n",
    "            if i.text.endswith('Jobs'):\n",
    "                company_temp = i.text.replace(' Jobs','')\n",
    "            else:\n",
    "                location_temp = i.text.replace('Jobs in ',' (')\n",
    "        \n",
    "        link_ext = job.a['href']\n",
    "        \n",
    "        #details = []\n",
    "        more_info = extract_inner(link_ext)\n",
    "        try:\n",
    "            divs_inner_1 = more_info.find('div',class_ = 'opportunity-preview__body').find_all('ul')\n",
    "            details = []\n",
    "        \n",
    "            for info in divs_inner_1:\n",
    "                for i in (info.find_all('li')):\n",
    "                    details.append(i.text.strip())\n",
    "            \n",
    "        except:\n",
    "            details= []\n",
    "            \n",
    "        try:\n",
    "            full_job = (more_info.find('h1',class_ = 'opportunity-preview__title').text)\n",
    "        except:\n",
    "            full_job = (title + '-' + company_temp + location_temp + ')')\n",
    "            # Up until - is job, after dash to ( is company and (INSIDE parenthese is location)\n",
    "        \n",
    "        job = {\n",
    "            'title': title,\n",
    "            'job_info': full_job,\n",
    "            'url': 'https://www.teamworkonline.com' + link_ext,\n",
    "            'details': details,\n",
    "            'scrape_datetime': datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        joblist.append(job)\n",
    "    \n",
    "    return\n",
    "\n",
    "joblist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f44c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ((1,3),(3,5),(5,7),(7,9),(9,11))\n",
    "for i in pages:\n",
    "    for j in range(i[0],i[1]):\n",
    "        c=extract(j)\n",
    "        transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## USED FOR INITIAL SCRAPE ###################################################\n",
    "# Creating and cleaning job data table\n",
    "job_posting_teamwork = pd.DataFrame(joblist)\n",
    "\n",
    "for i,j in job_posting_teamwork.iterrows():\n",
    "    if j['title'] in (j['job_info']):\n",
    "        j['job_info'] = j['job_info'].replace(j['title'],'')\n",
    "\n",
    "job_posting_teamwork[\"Location\"] = (job_posting_teamwork[\"job_info\"].str.partition(\"(\")[2]).str.replace(\")\",\"\").str.replace(' · ',', ').str.replace('· ',', ').str.strip()\n",
    "job_posting_teamwork[\"Company\"] = job_posting_teamwork[\"job_info\"].str.partition(\"(\")[0].str.partition(\"-\")[2].str.strip()\n",
    "\n",
    "job_posting_teamwork['job_city'] = job_posting_teamwork['Location'].str.partition(\",\")[0]\n",
    "job_posting_teamwork['job_state'] = job_posting_teamwork['Location'].str.partition(\",\")[2]\n",
    "\n",
    "job_posting_teamwork = job_posting_teamwork.drop([\"job_info\",\"Location\"],axis=1)\n",
    "for i,j in job_posting_teamwork.iterrows():\n",
    "    if(j[\"Company\"] == \"Oakland A's\"):\n",
    "        j[\"Company\"] = \"Oakland Athletics\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "number = LabelEncoder()\n",
    "\n",
    "job_posting_teamwork[\"company_ID\"] = number.fit_transform(job_posting_teamwork[\"Company\"].astype('str'))\n",
    "job_posting_teamwork.loc[job_posting_teamwork['company_ID'] == 0,'company_ID'] = (max(job_posting_teamwork['company_ID'])+1)\n",
    "\n",
    "job_posting_teamwork['job_ID'] = np.arange(1, len(job_posting_teamwork)+1)\n",
    "\n",
    "job_posting_teamwork['posting_source_ID'] = 2\n",
    "job_posting_teamwork['posting_datetime'] = 'NA'\n",
    "job_posting_teamwork['application_deadline'] = 'Unknown'\n",
    "job_posting_teamwork['salary'] = 'Unknown'\n",
    "job_posting_teamwork['scrape_datetime'] = pd.to_datetime(job_posting_teamwork['scrape_datetime'])\n",
    "\n",
    "job_posting_teamwork = job_posting_teamwork.rename(columns = {'title': 'job_title', 'url': 'posting_link'})\n",
    "job_posting_teamwork_df = job_posting_teamwork.reindex(columns = ['job_ID','job_title',\"company_ID\",'posting_source_ID','posting_datetime','scrape_datetime','application_deadline','salary','job_city','job_state','details','posting_link'])\n",
    "\n",
    "# Creating Company Table\n",
    "Company_Team = pd.DataFrame(job_posting_teamwork[['company_ID','Company']])\n",
    "Company_Team_df = Company_Team.drop_duplicates()\n",
    "\n",
    "# Creating the requirements table\n",
    "job_requirements_df = pd.DataFrame(job_posting_teamwork_df[['job_ID','details']])\n",
    "job_requirements_df_final = job_requirements_df.assign(temp = job_requirements_df.details.str.split(\",\")).explode('details').drop('temp',axis=1)\n",
    "job_requirements_df_final['details'] = job_requirements_df_final['details'].str.replace(\"'\",\"\").str.replace('\"','')\n",
    "job_posting_teamwork_df = job_posting_teamwork_df.drop('details',axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931aec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping leagues from wikipedia to get big team information\n",
    "url_page = requests.get('https://en.wikipedia.org/wiki/Major_League_Soccer')\n",
    "soup = BeautifulSoup(url_page.content,'html.parser')\n",
    "\n",
    "table_sec = soup.find('table',class_=\"wikitable sortable\")\n",
    "table_mls = table_sec.find_all('tr')\n",
    "company_mls_list = []\n",
    "\n",
    "for team in table_mls:\n",
    "    team_info = team.find_all('td')\n",
    "    company_info_mls = []\n",
    "    for info in team_info:\n",
    "        company_info_mls.append(info.text.strip())\n",
    "    company_mls = {\n",
    "        'total_info': company_info_mls\n",
    "    } \n",
    "\n",
    "    company_mls_list.append(company_mls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b964c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(company_mls_list)\n",
    "company_mls_df = pd.DataFrame(df1.total_info.tolist(), columns = ['team_name','Headquarters','Stadium','capacity','founded','joined','coach'])\n",
    "company_mls_df['league'] = 'Major League Soccer'\n",
    "company_mls_df['league_short'] = 'MLS'\n",
    "company_mls_df = company_mls_df.reindex(columns = ['team_name','Headquarters','league','league_short','Stadium','capacity','founded'])\n",
    "company_mls_df.loc[company_mls_df['team_name'] == 'LA Galaxy','team_name'] = 'Los Angeles Galaxy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page = requests.get('https://en.wikipedia.org/wiki/Major_League_Baseball')\n",
    "soup = BeautifulSoup(url_page.content,'html.parser')\n",
    "table_sec = soup.find('table',class_=\"wikitable sortable\")\n",
    "table_mlb = table_sec.find_all('tr')\n",
    "company_mlb_list = []\n",
    "\n",
    "for team in table_mlb:\n",
    "    team_info = team.find_all('td')\n",
    "    company_info_mlb = []\n",
    "    for info in team_info:\n",
    "        company_info_mlb.append(info.text.strip())\n",
    "    company_mlb = {\n",
    "        'total_info': company_info_mlb\n",
    "    } \n",
    "\n",
    "    company_mlb_list.append(company_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(company_mlb_list)\n",
    "company_mlb_df = pd.DataFrame(df1.total_info.tolist(), columns = ['team_name','Headquarters','Stadium','capacity','coordinates','founded','joined','reference'])\n",
    "company_mlb_df['league'] = 'Major League Baseball' \n",
    "company_mlb_df['league_short'] = 'MLB' \n",
    "company_mlb_df = company_mlb_df.reindex(columns = ['team_name','Headquarters','league','league_short','Stadium','capacity','founded'])\n",
    "company_mlb_df['founded'] = company_mlb_df['founded'].str.replace(r\"\\(..\\)\",'')\n",
    "company_mlb_df['founded'] = company_mlb_df['founded'].str.replace(\"*\",'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page = requests.get(\"https://en.wikipedia.org/wiki/National_Football_League\")\n",
    "soup = BeautifulSoup(url_page.content,'html.parser')\n",
    "table_sec = soup.find('table',class_=\"wikitable sortable plainrowheaders\")\n",
    "table_nfl = table_sec.find_all('tr')\n",
    "company_nfl_list = []\n",
    "\n",
    "for team in table_nfl:\n",
    "    team_info = team.find_all('td')\n",
    "    company_info_nfl = []\n",
    "    for info in team_info:\n",
    "        company_info_nfl.append(info.text.strip())\n",
    "    company_nfl = {\n",
    "        'total_info': company_info_nfl\n",
    "    } \n",
    "\n",
    "    company_nfl_list.append(company_nfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(company_nfl_list)\n",
    "company_nfl_df = pd.DataFrame(df1.total_info.tolist(), columns = ['team_name','Headquarters','Stadium','capacity','coordinates','founded','coach'])\n",
    "company_nfl_df['league'] = 'National Football League'\n",
    "company_nfl_df['league_short'] = 'NFL'\n",
    "company_nfl_df = company_nfl_df.reindex(columns = ['team_name','Headquarters','league','league_short','Stadium','capacity','founded'])\n",
    "company_nfl_df['founded'] = company_nfl_df['founded'].str.partition('(')[0]\n",
    "company_nfl_df['Stadium'] = company_nfl_df['Stadium'].str.replace(r\"\\[.\\]\",'')\n",
    "company_nfl_df['founded'] = company_nfl_df['founded'].str.replace(r\"\\[.\\]\",'')\n",
    "company_nfl_df['team_name'] = company_nfl_df['team_name'].str.replace(\"*\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page = requests.get(\"https://en.wikipedia.org/wiki/National_Hockey_League\")\n",
    "soup = BeautifulSoup(url_page.content,'html.parser')\n",
    "table_sec = soup.find('table',class_=\"wikitable\")\n",
    "table_nhl = table_sec.find_all('tr')\n",
    "company_nhl_list = []\n",
    "\n",
    "for team in table_nhl:\n",
    "    team_info = team.find_all('td')\n",
    "    company_info_nhl = []\n",
    "    for info in team_info:\n",
    "        company_info_nhl.append(info.text.strip())\n",
    "    company_nhl = {\n",
    "        'total_info': company_info_nhl\n",
    "    } \n",
    "\n",
    "    company_nhl_list.append(company_nhl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(company_nhl_list)\n",
    "company_nhl_df = pd.DataFrame(df1.total_info.tolist(), columns = ['team_name','Headquarters','Stadium','capacity','founded','joined','manager','coach','captain'])\n",
    "company_nhl_df['league'] = 'National Hockey League'\n",
    "company_nhl_df['league_short'] = 'NHL'\n",
    "company_nhl_df = company_nhl_df.reindex(columns = ['team_name','Headquarters','league','league_short','Stadium','capacity','founded'])\n",
    "company_nhl_df['founded'] = company_nhl_df['founded'].str.replace(\"*\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page = requests.get(\"https://en.wikipedia.org/wiki/National_Basketball_Association\")\n",
    "soup = BeautifulSoup(url_page.content,'html.parser')\n",
    "table_sec = soup.find('table',class_=\"wikitable\")\n",
    "table_nba = table_sec.find_all('tr')\n",
    "company_nba_list = []\n",
    "\n",
    "for team in table_nba:\n",
    "    team_info = team.find_all('td')\n",
    "    company_info_nba = []\n",
    "    for info in team_info:\n",
    "        company_info_nba.append(info.text.strip())\n",
    "    company_nba = {\n",
    "        'total_info': company_info_nba\n",
    "    } \n",
    "\n",
    "    company_nba_list.append(company_nba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(company_nba_list)\n",
    "company_nba_df = pd.DataFrame(df1.total_info.tolist(), columns = ['team_name','Headquarters','Stadium','capacity','coordinates','founded','joined'])\n",
    "company_nba_df['league'] = 'National Baseball Association'\n",
    "company_nba_df['league_short'] = 'NBA'\n",
    "company_nba_df = company_nba_df.reindex(columns = ['team_name','Headquarters','league','league_short','Stadium','capacity','founded'])\n",
    "\n",
    "company_nba_df.loc[4,'capacity'] = '19,812'\n",
    "company_nba_df.loc[4,'Stadium'] = 'Madison Square Garden'\n",
    "company_nba_df.loc[4,'Headquarters'] = 'New York City, New York'\n",
    "company_nba_df.loc[4,'founded'] = '1946'\n",
    "\n",
    "company_nba_df.loc[25,'capacity'] = '19,079'\n",
    "company_nba_df.loc[25,'Stadium'] = 'Crypto.com Arena'\n",
    "company_nba_df.loc[25,'Headquarters'] = 'Los Angeles, California'\n",
    "company_nba_df.loc[25,'founded'] = '1947'\n",
    "company_nba_df['founded'] = company_nba_df['founded'].str.replace(\"*\",'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17912c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY RUN ONCE!!!!! ###\n",
    "company_mlb_df.drop(index=company_mlb_df.index[[0,1,17]], axis=0, inplace=True)\n",
    "company_nfl_df.drop(index=company_nfl_df.index[[0,1,18,35]], axis=0, inplace=True)\n",
    "company_nba_df.drop(index=company_nba_df.index[[0,1,17]], axis=0, inplace=True)\n",
    "company_nhl_df.drop(index=company_nhl_df.index[[0,1,18]], axis=0, inplace=True)\n",
    "company_mls_df.drop(index=company_mls_df.index[[0,1,17]], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be382c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_teams_df_temp = pd.concat([company_mlb_df,\n",
    "company_nfl_df,\n",
    "company_nba_df,\n",
    "company_nhl_df,\n",
    "company_mls_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_Team_df_temp2 = pd.merge(Company_Team_df, company_teams_df_temp, left_on=\"Company\", right_on=\"team_name\", how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = max(Company_Team_df['company_ID'])\n",
    "new_ID = count + 1\n",
    "\n",
    "Company_Team_df_temp2['company_ID'] = np.where(Company_Team_df_temp2['company_ID']>0,Company_Team_df_temp2['company_ID'],0)\n",
    "Company_Team_df_temp2['Company'] = np.where(Company_Team_df_temp2['Company'].isnull(),'None',Company_Team_df_temp2['Company'])\n",
    "Company_Team_df_temp2['capacity'] = Company_Team_df_temp2['capacity'].str.replace(\",\",'')\n",
    "Company_Team_df_temp2['capacity'] = np.where(Company_Team_df_temp2['capacity'].isnull(),0,Company_Team_df_temp2['capacity'])\n",
    "Company_Team_df_temp2['founded'] = np.where(Company_Team_df_temp2['founded'].isnull(),0,Company_Team_df_temp2['founded'])\n",
    "\n",
    "for i,j in Company_Team_df_temp2.iterrows():\n",
    "    if (j['company_ID'] == 0.0):\n",
    "        Company_Team_df_temp2.at[i,'company_ID'] = new_ID\n",
    "        new_ID = new_ID + 1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i,j in Company_Team_df_temp2.iterrows():\n",
    "    if (j['Company']=='None'):\n",
    "        Company_Team_df_temp2.at[i,'Company'] = j['team_name']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "Company_Team_df_temp2['company_ID'] = Company_Team_df_temp2['company_ID'].astype(int)\n",
    "Company_Team_df_temp2['founded'] = Company_Team_df_temp2['founded'].astype(int)\n",
    "Company_Team_df_temp2['capacity'] = Company_Team_df_temp2['capacity'].astype(int)\n",
    "\n",
    "Company_Team_df_final = Company_Team_df_temp2.drop('team_name',axis = 1)\n",
    "Company_Team_df_final = Company_Team_df_final.fillna('NA')\n",
    "Company_Team_df_final['Headquarters_city'] = Company_Team_df_final['Headquarters'].str.partition(', ')[0]\n",
    "Company_Team_df_final['Headquarters_state'] = Company_Team_df_final['Headquarters'].str.partition(', ')[2]\n",
    "Company_Team_df_final.drop('Headquarters',axis = 1,inplace = True)\n",
    "Company_Team_df_final = Company_Team_df_final.reindex(columns = ['company_ID', 'Company', 'league', 'league_short', 'Headquarters_city', 'Headquarters_state', 'Stadium', 'capacity', 'founded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06083036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL Command Execution Begins Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources = pd.DataFrame({'source_ID': [2], 'source_name': ['Teamwork Online']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6093e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize connection to MYSQL\n",
    "database = MySQLdb.connect(host=\"localhost\" , user=\"root\" , passwd=\"Pps11844\")\n",
    "cursor = database.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_statement):\n",
    "    try:\n",
    "        cursor.execute(query_statement);\n",
    "        database.commit();\n",
    "        print(\"Data is Succefully Inserted\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        database.rollback();\n",
    "        print(\"The  Exception Occured : \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdfcf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execute_query(\"USE JobsinSports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"CREATE TABLE IF NOT EXISTS Job_Posting(job_ID BIGINT PRIMARY KEY NOT NULL UNIQUE, job_title VARCHAR(255), company_ID INT, posting_datetime DATETIME, scraped_datetime DATETIME, application_deadline DATETIME, salary VARCHAR(155), job_city VARCHAR(155), job_state VARCHAR(155), posting_url TEXT, source_identifier INT DEFAULT 0);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eefb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"CREATE TABLE IF NOT EXISTS Company_Team(company_ID INT PRIMARY KEY NOT NULL UNIQUE, company_name VARCHAR(255),company_headquarters_city VARCHAR(255), company_headquarters_state VARCHAR(255), league VARCHAR(100), league_short VARCHAR(55), stadium VARCHAR(55), stadium_capacity INT, founded_year INT);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"CREATE TABLE IF NOT EXISTS Job_Requirements(job_ID BIGINT, requirements TEXT,PRIMARY KEY(job_ID,requirements(255)));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in job_requirements_df_final.iterrows():\n",
    "    execute_query('INSERT INTO Job_Requirements (job_ID, requirements) VALUES (%d, \"%s\")' % (j['job_ID'],j['details']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Sources.iterrows():\n",
    "    execute_query('INSERT INTO Sources (source_ID, source_name) VALUES (%d, \"%s\")' % (j['source_ID'],j['source_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc8a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,j in Company_Team_df_final.iterrows():\n",
    "    execute_query('INSERT INTO Company_Team (company_ID, company_name, company_headquarters_city, company_headquarters_state, league, league_short, stadium, stadium_capacity, founded_year) VALUES (%d, \"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",%d,%d)' % (j['company_ID'], j['Company'], j['Headquarters_city'], j['Headquarters_state'], j['league'], j['league_short'], j['Stadium'], j['capacity'], j['founded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43e768",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,j in job_posting_teamwork_df.iterrows():\n",
    "    execute_query('INSERT INTO Job_Posting (job_ID, job_title, company_ID, scraped_datetime, job_city, job_state, posting_url, source_identifier) VALUES (%d, \"%s\", %d, \"%s\", \"%s\", \"%s\", \"%s\", %d)' % (j['job_ID'], j['job_title'], j['company_ID'], j['scrape_datetime'], j['job_city'], j['job_state'], j['posting_link'], j['posting_source_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only used to delete and edit schema during Debugging phase \n",
    "#execute_query(\"DROP TABLE Job_Requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c27aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
