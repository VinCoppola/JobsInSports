{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de887d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "from lxml import html\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install requests_html\n",
    "#from requests_html import HTMLSession\n",
    "import random\n",
    "import re\n",
    "#from nltk import bigrams\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "#! pip install wordcloud\n",
    "#from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dict1, dict2):\n",
    "    return(dict2.update(dict1))\n",
    "    \n",
    "def extract(league):\n",
    "    url = f'https://www.linkedin.com/jobs/search?keywords={league}&location=United%20States&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r = requests.get(url,headers)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def extract_inner(link_ext):\n",
    "    url_inner = link_ext\n",
    "    \n",
    "    user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': random.choice(user_agents_list)}\n",
    "\n",
    "    r_inner = requests.get(url_inner,headers)\n",
    "    soup_inner = BeautifulSoup(r_inner.content,'html.parser')\n",
    "    return(soup_inner)\n",
    "\n",
    "def transform(soup):\n",
    "    divs = soup.find_all('div',class_ = 'base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "    \n",
    "    for job in divs:\n",
    "        title = job.find('h3',class_='base-search-card__title').text.strip()\n",
    "        company = job.find('h4',class_ = 'base-search-card__subtitle').text.strip()\n",
    "        location = job.find('span',class_ = 'job-search-card__location').text.strip()\n",
    "        \n",
    "        scrape_date = datetime.now()\n",
    "        try:\n",
    "            posting_delta = job.find('time',class_ = 'job-search-card__listdate').text.strip().partition(' ')\n",
    "            delta = posting_delta[0]\n",
    "\n",
    "            if(posting_delta[2].partition(' ')[0] == 'hours'):\n",
    "                post_date = scrape_date - timedelta(hours = int(delta))\n",
    "        \n",
    "            elif(posting_delta[2].partition(' ')[0] == 'days'):\n",
    "                post_date = scrape_date - timedelta(days = int(delta))\n",
    "        \n",
    "            elif(posting_delta[2].partition(' ')[0] == 'weeks'):\n",
    "                post_date = scrape_date - timedelta(weeks = int(delta))\n",
    "        \n",
    "            else:\n",
    "                delta = delta * 4\n",
    "                post_date = scrape_date - timedelta(weeks = int(delta))\n",
    "        except:\n",
    "            post_date = datetime.now()\n",
    "            \n",
    "        link_ext = job.a['href']\n",
    "        job_ID = link_ext.partition('?refId')[0].split('-')[-1]\n",
    "        \n",
    "        #details = []\n",
    "        more_info = extract_inner(link_ext)\n",
    "        try:\n",
    "            divs_inner = more_info.find('div',class_ = 'show-more-less-html__markup')\n",
    "            divs_inner_1 = divs_inner.find_all('ul')\n",
    "            base_text = divs_inner.prettify()\n",
    "            details = []\n",
    "        \n",
    "            for info in divs_inner_1:\n",
    "                for i in (info.find_all('li')):\n",
    "                    details.append(i.text.strip())\n",
    "            \n",
    "        except:\n",
    "            details= []\n",
    "        \n",
    "        job = {\n",
    "            'job_ID': job_ID,\n",
    "            'title': title,\n",
    "            'Location': location,\n",
    "            'Company': company,\n",
    "            'details': details,\n",
    "            'url': link_ext,\n",
    "            'posting_datetime': post_date.strftime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "            'scrape_datetime': scrape_date.strftime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "            'additionals': base_text\n",
    "        }\n",
    "        \n",
    "        joblist.append(job)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist = []\n",
    "leagues = ['Major%20League%20Soccer', 'Major%20League%20Baseball','National%20Football%20League']\n",
    "for league in leagues:\n",
    "    c=extract(league)\n",
    "    transform(c)\n",
    "    \n",
    "joblist1 = joblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist = []\n",
    "leagues_again = ['National%20Hockey%20League', 'National%20Basketball%20Association']\n",
    "for league in leagues_again:\n",
    "    c=extract(league)\n",
    "    transform(c)\n",
    "    \n",
    "joblist2 = joblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af27b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = MySQLdb.connect(host=\"localhost\" , user=\"root\" , passwd=\"Pps11844\")\n",
    "cursor = database.cursor()\n",
    "\n",
    "def execute_query(query_statement):\n",
    "    try:\n",
    "        cursor.execute(query_statement);\n",
    "        database.commit();\n",
    "        print(\"Data is Succefully Inserted\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        database.rollback();\n",
    "        print(\"The  Exception Occured : \", e)\n",
    "\n",
    "execute_query(\"USE JobsinSports\")\n",
    "\n",
    "SQL_df_posting = pd.read_sql('select * from job_posting',database)\n",
    "\n",
    "SQL_df_companies = pd.read_sql('select * from company_team',database)\n",
    "\n",
    "cursor.execute(\"SELECT MAX(company_ID) FROM company_team;\")\n",
    "result = cursor.fetchone();\n",
    "max_comp_ID = result[0]\n",
    "\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_linkedin_1 = pd.DataFrame(joblist1)\n",
    "job_posting_linkedin_2 = pd.DataFrame(joblist2)\n",
    "job_posting_linkedin = pd.concat([job_posting_linkedin_1, job_posting_linkedin_2])\n",
    "job_posting_linkedin.reset_index(drop=True, inplace=True)\n",
    "\n",
    "job_posting_linkedin['job_ID'] = job_posting_linkedin['job_ID'].astype(float)\n",
    "\n",
    "for i,j in job_posting_linkedin.iterrows():\n",
    "    if(re.findall(r'\\$',j['additionals'])):\n",
    "        job_posting_linkedin.at[i,'salary'] = '$'+(j['additionals'].partition('$')[2].partition('.')[0].partition('<')[0].partition('/')[0].partition('\\\\')[0].replace(' ','').replace('to','-').replace('TO','-').replace('K',',000').replace('k',',000').replace('\\n',',000'))\n",
    "    else:\n",
    "        job_posting_linkedin.at[i,'salary'] = 'NA'\n",
    "\n",
    "for i,j in job_posting_linkedin.iterrows():\n",
    "    if(len(j['salary'])==3):\n",
    "        job_posting_linkedin.at[i,'salary'] = (j['salary'] + '/hr')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for i,j in job_posting_linkedin.iterrows():\n",
    "    if(re.findall(r'New York City',j['Location'])):\n",
    "        job_posting_linkedin.at[i,'Location'] = 'New York, NY'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "job_posting_linkedin['job_city'] = job_posting_linkedin['Location'].str.partition(\",\")[0]\n",
    "job_posting_linkedin['job_state'] = job_posting_linkedin['Location'].str.partition(\",\")[2]\n",
    "\n",
    "job_posting_linkedin['Company'] = job_posting_linkedin['Company'].str.partition('(')[0].str.replace('Football Club ','FC').str.replace('Football Club','FC').str.replace('Soccer Club','SC')\n",
    "job_posting_linkedin['Company'] = job_posting_linkedin['Company'].str.strip()\n",
    "job_posting_linkedin['posting_source_ID'] = 3\n",
    "job_posting_linkedin['application_deadline'] = 'Unknown'\n",
    "job_posting_linkedin['scrape_datetime'] = pd.to_datetime(job_posting_linkedin['scrape_datetime'])\n",
    "job_posting_linkedin['posting_datetime'] = pd.to_datetime(job_posting_linkedin['posting_datetime'])\n",
    "\n",
    "\n",
    "job_requirements_df = pd.DataFrame(job_posting_linkedin[['job_ID','details']])\n",
    "job_requirements_df_final = job_requirements_df.assign(temp = job_requirements_df.details.str.split(\",\")).explode('details').drop('temp',axis=1)\n",
    "job_requirements_df_final['details'] = job_requirements_df_final['details'].str.replace(\"'\",\"\").str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_linkedin.drop(['Location','details','additionals'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9f50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_Team = pd.DataFrame(job_posting_linkedin['Company'])\n",
    "Company_Team_df = Company_Team.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2947c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_Team_df['Company_temp'] = [1,2,3,4,5,6,7,8]\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 1,'company_ID'] = int(max_comp_ID + 1)\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 5,'company_ID'] = int(max_comp_ID + 2)\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 6,'company_ID'] = int(max_comp_ID + 3)\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 7,'company_ID'] = int(max_comp_ID + 4)\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 8,'company_ID'] = int(max_comp_ID + 5)\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 2,'company_ID'] = 258\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 3,'company_ID'] = 257\n",
    "Company_Team_df.loc[Company_Team_df['Company_temp'] == 4,'company_ID'] = 255\n",
    "Company_Team_df.drop('Company_temp',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_Team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_linkedin_df = pd.merge(job_posting_linkedin, Company_Team_df, left_on=\"Company\", right_on=\"Company\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_linkedin_df = job_posting_linkedin_df.reindex(columns = ['job_ID','title',\"company_ID\",'posting_source_ID','posting_datetime','scrape_datetime','application_deadline', 'salary','job_city','job_state','url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources = pd.DataFrame({'source_ID': [3], 'source_name': ['Linkedin']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested but not perfected yet -- IGNORE\n",
    "#job_posting_linkedin_df\n",
    "#count = 1\n",
    "\n",
    "#for i,j in Company_Team_df.iterrows():\n",
    "#  print(i, j['Company'])\n",
    "#    if (j['Company'] in SQL_df_companies['company_name'].values):\n",
    "#        j.at[i,'company_ID'] = SQL_df_companies['company_ID']\n",
    "#    else:\n",
    "#        Company_Team_df.at[i,'company_ID'] = max_comp_ID + count\n",
    "#        count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize connection to MYSQL\n",
    "database = MySQLdb.connect(host=\"localhost\" , user=\"root\" , passwd=\"Pps11844\")\n",
    "cursor = database.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_statement):\n",
    "    try:\n",
    "        cursor.execute(query_statement);\n",
    "        database.commit();\n",
    "        print(\"Data is Succefully Inserted\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        database.rollback();\n",
    "        print(\"The  Exception Occured : \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357217e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"USE JobsinSports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7976ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in job_requirements_df_final.iterrows():\n",
    "    execute_query('INSERT INTO Job_Requirements (job_ID, requirements) VALUES (%d, \"%s\")' % (j['job_ID'],j['details']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Sources.iterrows():\n",
    "    execute_query('INSERT INTO Sources (source_ID, source_name) VALUES (%d, \"%s\")' % (j['source_ID'],j['source_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ba481",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Company_Team_df.iterrows():\n",
    "    execute_query('INSERT INTO Company_Team (company_ID, company_name) VALUES (%d, \"%s\")' % (j['company_ID'], j['Company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in job_posting_linkedin_df.iterrows():\n",
    "    execute_query('INSERT INTO Job_Posting (job_ID, job_title, company_ID, posting_datetime, scraped_datetime, salary, job_city, job_state, posting_url, source_identifier) VALUES (%d, \"%s\", %d, \"%s\",\"%s\", \"%s\", \"%s\", \"%s\", \"%s\", %d)' % (j['job_ID'], j['title'], j['company_ID'], j['posting_datetime'], j['scrape_datetime'], j['salary'], j['job_city'], j['job_state'], j['url'], j['posting_source_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d29ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
